{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfd1ae6-a641-4b58-88ac-f0ec994390a6",
   "metadata": {},
   "source": [
    "# Forecasting the Sales of a Supermarket During a Festive Season"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f36af-9f13-40bd-9cba-a2d3e7a179da",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b59d9-21c8-4771-ac4a-b83a04fad731",
   "metadata": {},
   "source": [
    "* TODO: Write down the problem explicitly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1f1a7-49a0-4905-b39e-1aa84e5b52c3",
   "metadata": {},
   "source": [
    "## Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fe02e-7171-4146-bfb7-1b51c7e9b9bc",
   "metadata": {},
   "source": [
    "* We use the dataset provided by [Walmart Recruiting - Store Sales Forecasting](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data) challenge. It is the historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments. We will use this data to predict the department-wise sales for each store in the Christmas.\n",
    "\n",
    "* The dataset consists of 4 files (TODO: Add file description)\n",
    "    * `stores.csv`\n",
    "    * `train.csv`\n",
    "    * `test.csv`\n",
    "    * `features.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd2eb9-c1d4-434e-ba6a-38daace45416",
   "metadata": {},
   "source": [
    "## EDA and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bfd7a879-481f-4ced-bcf6-f167328e0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import types\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d25f801f-b48c-4b6a-a0e9-66faf63c6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32a9ef2-afd8-4d82-b078-2d1144b1b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df23ffd-95ab-4057-896a-d69a20519181",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"Business Intelligence Capstone\")\n",
    "         .master(\"local[*]\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8019fce-dbfd-4580-8b78-f1ec5bb0527f",
   "metadata": {},
   "source": [
    "* Load the Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb4aef8-3c1d-4d2d-851c-e0b44b0d9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_schema = types.StructType([\n",
    "  types.StructField(\"Store\", types.StringType(), nullable=False),\n",
    "  types.StructField(\"Type\", types.StringType(), nullable=False),\n",
    "  types.StructField(\"Size\", types.IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "\n",
    "store_df = (spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"schema\", store_schema)\n",
    "            .csv(\"../data/stores.csv\")\n",
    "            .withColumnRenamed(\"Store\", \"store\")\n",
    "            .withColumnRenamed(\"Type\", \"type\")\n",
    "            .withColumnRenamed(\"Size\", \"size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b288c06-f9cd-4011-b7e5-5ceb54668a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|store|type|  size|\n",
      "+-----+----+------+\n",
      "|    1|   A|151315|\n",
      "|    2|   A|202307|\n",
      "|    3|   B| 37392|\n",
      "|    4|   A|205863|\n",
      "|    5|   B| 34875|\n",
      "+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba0d948-526f-477b-9e98-13f37cf2a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.createOrReplaceTempView(\"stores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6a256-3909-407b-8ea4-39910b0f1a0d",
   "metadata": {},
   "source": [
    "* Load the Train/Test and Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5215b70d-dfaf-4308-ac37-298cc1c931e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.36 ms, sys: 4.53 ms, total: 13.9 ms\n",
      "Wall time: 1.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = (spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(\"../data/train.csv\")\n",
    "            .select(\n",
    "              col(\"Store\").cast(types.StringType()).alias(\"store\"),\n",
    "              col(\"Dept\").cast(types.StringType()).alias(\"dept\"),\n",
    "              col(\"Date\").cast(types.DateType()).alias(\"date\"),\n",
    "              col(\"Weekly_Sales\").alias(\"weekly_sales\"),\n",
    "              col(\"IsHoliday\").alias(\"is_holiday\")\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "505fbec4-734e-498f-9836-c0b74642e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- weekly_sales: double (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67a27a8a-4161-4dc0-8851-bdb2c2974070",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.createOrReplaceTempView(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef4a60cc-a3ea-4803-92c0-77c8011cc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = (spark.read\n",
    "               .option(\"header\", \"true\")\n",
    "               .option(\"inferSchema\", \"true\")\n",
    "               .csv(\"../data/features.csv\")\n",
    "               .select(\n",
    "                 col(\"Store\").cast(types.StringType()).alias(\"store\"),\n",
    "                 col(\"Date\").cast(types.DateType()).alias(\"date\"),\n",
    "                 col(\"Temperature\").alias(\"temperature\"),\n",
    "                 col(\"Fuel_Price\").alias(\"fuel_price\"),\n",
    "                 col(\"MarkDown1\").cast(types.DoubleType()).alias(\"markdown_1\"),\n",
    "                 col(\"MarkDown2\").cast(types.DoubleType()).alias(\"markdown_2\"),\n",
    "                 col(\"MarkDown3\").cast(types.DoubleType()).alias(\"markdown_3\"),\n",
    "                 col(\"MarkDown4\").cast(types.DoubleType()).alias(\"markdown_4\"),\n",
    "                 col(\"MarkDown5\").cast(types.DoubleType()).alias(\"markdown_5\"),\n",
    "                 col(\"CPI\").alias(\"cpi\"),\n",
    "                 col(\"Unemployment\").cast(types.DoubleType()).alias(\"unemployment\"),\n",
    "                 col(\"IsHoliday\").alias(\"is_holiday\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2884828-fd5b-440e-9acc-95730088315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.createOrReplaceTempView(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7ec848d-12e2-4385-a73b-f2691489d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- fuel_price: double (nullable = true)\n",
      " |-- markdown_1: double (nullable = true)\n",
      " |-- markdown_2: double (nullable = true)\n",
      " |-- markdown_3: double (nullable = true)\n",
      " |-- markdown_4: double (nullable = true)\n",
      " |-- markdown_5: double (nullable = true)\n",
      " |-- cpi: string (nullable = true)\n",
      " |-- unemployment: double (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b24ed-5d18-4b0e-8cc7-02b97d90349e",
   "metadata": {},
   "source": [
    "* Merge `train` with `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c9dda74-529d-4ef4-a6aa-8592e3ec5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = spark.sql(\n",
    "  \"\"\"\n",
    "  select t.store, t.date,\n",
    "         t.dept, t.weekly_sales,\n",
    "         f.temperature, f.fuel_price, \n",
    "         f.markdown_1, f.markdown_2, f.markdown_3, f.markdown_4, f.markdown_5,\n",
    "         f.cpi, f.unemployment, f.is_holiday\n",
    "  from train t inner join features f\n",
    "  on (t.store = f.store and t.date = f.date)\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d61839a5-31e1-4f14-8883-e47ea6545c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.createOrReplaceTempView(\"train_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "771dc49f-d5a1-498b-8a8c-352a668dc84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|day_of_week|\n",
      "+-----------+\n",
      "|     Friday|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "  select distinct date_format(date, \"EEEE\") as day_of_week\n",
    "  from train_features\n",
    "  \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20502e1e-1f4c-479a-a6ae-f2012534fd00",
   "metadata": {},
   "source": [
    "* The `date` field does not represent the day, but the friday of every week in a year. We will create two field `year` and `week` to make the problem more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d3cad3a-5390-42ea-a3ec-1cf768c6e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = (train_features\n",
    "                  .withColumn(\"year\", date_format(col(\"date\"), \"y\"))\n",
    "                  .withColumn(\"week\", date_format(col(\"date\"), \"w\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8bdcccb0-bce8-412e-b653-3e3e5be8353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.createOrReplaceTempView(\"train_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f24eb2b-939f-4f0a-8483-467ff4fa8650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2012|\n",
      "|2011|\n",
      "|2010|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "  select distinct year\n",
    "  from train_features\n",
    "  \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c855105-4ba4-46d5-b170-5ac7235848c2",
   "metadata": {},
   "source": [
    "* Next, we will have to mark Christmas weeks (a Christmas week is a week containing the Christmas day). Christmas is always on December 25th so we will mark the week containing `2010-12-25`, `2011-12-25`, `2012-12-25` days as Christmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5244743b-adfc-4ebe-ac4d-dfb08bf5823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2010, week=51\n",
      "year=2011, week=51\n",
      "year=2012, week=52\n"
     ]
    }
   ],
   "source": [
    "print(\"year=2010, week={}\".format(date(2010, 12, 25).isocalendar()[1]))\n",
    "print(\"year=2011, week={}\".format(date(2011, 12, 25).isocalendar()[1]))\n",
    "print(\"year=2012, week={}\".format(date(2012, 12, 25).isocalendar()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e1c7eae-d5a5-41de-8f00-500788e0a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = spark.sql(\n",
    "  \"\"\"\n",
    "  select *,\n",
    "         case when year = 2010 and week = 51 then true\n",
    "              when year = 2011 and week = 51 then true\n",
    "              when year = 2012 and week = 52 then true\n",
    "              else false\n",
    "         end as is_christmas\n",
    "  from train_features\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95913194-3a9a-4694-a6a4-52289c61629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5972"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.where(col(\"is_christmas\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf818e-704d-4c41-9f5f-79f1249ee468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79179957-845b-47a6-9bc9-c79e5e0cfb96",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415fdbd-f93c-4a23-9ec8-e92bf54f05be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b327c6-2204-4bf3-8425-71aada56be6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
